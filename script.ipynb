{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00c933",
   "metadata": {},
   "source": [
    "**GITHUB REPO LINK:**\n",
    "https://github.com/UmarRasheed007/mlp-hidden-layer-study#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38717e25",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b891fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import pickle\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b171e0a",
   "metadata": {},
   "source": [
    "**Defaults**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eaa52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['grid.linewidth'] = 0.8\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.fancybox'] = False\n",
    "\n",
    "RND = 42\n",
    "np.random.seed(RND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16b9be",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad573fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [4, 8, 16, 32, 64, 128]\n",
    "epochs = 40\n",
    "n_samples = 1200\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "output_dir = \"mlp_width_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e47b7",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cc4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=n_samples, n_features=n_features,\n",
    "                           n_informative=10, n_redundant=5, n_classes=n_classes,\n",
    "                           class_sep=1.2, random_state=RND)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=RND, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103e8da",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0bb20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training width=4\n",
      "Training width=8\n",
      "Training width=16\n",
      "Training width=32\n",
      "Training width=64\n",
      "Training width=128\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "for w in widths:\n",
    "    print(f\"Training width={w}\")\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(w,), activation='relu', solver='adam',\n",
    "                        learning_rate_init=0.001, max_iter=1, warm_start=True, random_state=RND)\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    losses = []\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_acc.append(np.mean(clf.predict(X_train) == y_train))\n",
    "    test_acc.append(np.mean(clf.predict(X_test) == y_test))\n",
    "    losses.append(clf.loss_ if hasattr(clf, \"loss_\") else np.nan)\n",
    "    for ep in range(1, epochs):\n",
    "        clf.partial_fit(X_train, y_train)\n",
    "        train_acc.append(np.mean(clf.predict(X_train) == y_train))\n",
    "        test_acc.append(np.mean(clf.predict(X_test) == y_test))\n",
    "        losses.append(clf.loss_ if hasattr(clf, \"loss_\") else np.nan)\n",
    "    results[w] = {\"train_acc\": np.array(train_acc),\n",
    "                  \"test_acc\": np.array(test_acc),\n",
    "                  \"loss\": np.array(losses)}\n",
    "\n",
    "with open(os.path.join(output_dir, \"mlp_width_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd7e57",
   "metadata": {},
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16338648",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_paths = []\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(widths)))\n",
    "\n",
    "for i, w in enumerate(widths):\n",
    "    epochs_range = np.arange(1, epochs+1)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=100)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax[0].plot(epochs_range, results[w][\"train_acc\"], 'o-', linewidth=2, \n",
    "               markersize=4, label=\"Train\", color='steelblue', alpha=0.8)\n",
    "    ax[0].plot(epochs_range, results[w][\"test_acc\"], 's-', linewidth=2, \n",
    "               markersize=4, label=\"Test\", color='coral', alpha=0.8)\n",
    "    ax[0].set_title(f'Accuracy (Width = {w})', fontsize=12, fontweight='bold')\n",
    "    ax[0].set_xlabel('Epoch', fontsize=11)\n",
    "    ax[0].set_ylabel('Accuracy', fontsize=11)\n",
    "    ax[0].legend(loc='lower right', fontsize=10)\n",
    "    ax[0].grid(True, alpha=0.3, linestyle='--')\n",
    "    ax[0].set_ylim([0, 1.05])\n",
    "    \n",
    "    # Loss plot\n",
    "    ax[1].semilogy(epochs_range, results[w][\"loss\"], 'o-', linewidth=2, \n",
    "                   markersize=4, color='darkgreen', alpha=0.8)\n",
    "    ax[1].set_title(f'Training Loss (Width = {w})', fontsize=12, fontweight='bold')\n",
    "    ax[1].set_xlabel('Epoch', fontsize=11)\n",
    "    ax[1].set_ylabel('Loss (log scale)', fontsize=11)\n",
    "    ax[1].grid(True, alpha=0.3, linestyle='--', which='both')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    p = os.path.join(output_dir, f\"acc_loss_w{w}.png\")\n",
    "    fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    figure_paths.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83bf41e",
   "metadata": {},
   "source": [
    "**Additional Analysis Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946c2ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umarr\\AppData\\Local\\Temp\\ipykernel_18900\\2004957305.py:43: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  bp = ax.boxplot([X_train[:, i] for i in range(n_features)], labels=[f'F{i}' for i in range(n_features)],\n"
     ]
    }
   ],
   "source": [
    "# 1. Class Distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5), dpi=100)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "bars = ax.bar(unique, counts, color=colors[:len(unique)], edgecolor='black', \n",
    "              linewidth=1.5, alpha=0.8, width=0.6)\n",
    "ax.set_xlabel('Class Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training Set Class Distribution', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_xticks(unique)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"class_distribution.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)\n",
    "\n",
    "# 2. Feature Histograms\n",
    "fig, axes = plt.subplots(4, 5, figsize=(14, 10), dpi=100)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_features):\n",
    "    axes[i].hist(X_train[:, i], bins=25, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Feature {i}', fontsize=9, fontweight='bold')\n",
    "    axes[i].set_xlabel('Value', fontsize=8)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=8)\n",
    "    axes[i].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    axes[i].tick_params(labelsize=7)\n",
    "\n",
    "plt.suptitle('Feature Distributions (Training Set)', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"feature_histograms.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)\n",
    "\n",
    "# 3. Feature Box Plots\n",
    "fig, ax = plt.subplots(figsize=(14, 5), dpi=100)\n",
    "bp = ax.boxplot([X_train[:, i] for i in range(n_features)], labels=[f'F{i}' for i in range(n_features)],\n",
    "                 patch_artist=True, widths=0.6, notch=False)\n",
    "for patch, color in zip(bp['boxes'], plt.cm.tab20(np.linspace(0, 1, n_features))):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(bp[element], color='black', linewidth=1.2)\n",
    "ax.set_ylabel('Standardized Value', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Features', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Box Plots (Training Set)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"feature_boxplots.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)\n",
    "\n",
    "# 4. Correlation Heatmap\n",
    "corr_matrix = np.corrcoef(X_train.T)\n",
    "fig, ax = plt.subplots(figsize=(10, 9), dpi=100)\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xticks(np.arange(n_features))\n",
    "ax.set_yticks(np.arange(n_features))\n",
    "ax.set_xticklabels([f'F{i}' for i in range(n_features)], fontsize=8)\n",
    "ax.set_yticklabels([f'F{i}' for i in range(n_features)], fontsize=8)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Correlation', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=13, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"correlation_heatmap.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)\n",
    "\n",
    "# 5. Train-Test Scatter Plot (PCA projection)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=100)\n",
    "scatter_train = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, \n",
    "                           cmap='tab10', s=30, alpha=0.6, label='Train', edgecolors='black', linewidth=0.5)\n",
    "scatter_test = ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, \n",
    "                          cmap='tab10', s=60, alpha=0.8, label='Test', marker='^', \n",
    "                          edgecolors='darkred', linewidth=0.7)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Train-Test Data Distribution (PCA Projection)', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"train_test_scatter.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f587ce",
   "metadata": {},
   "source": [
    "**Summary: final accuracy with error bars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8aaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_acc = [results[w][\"test_acc\"][-1] for w in widths]\n",
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=100)\n",
    "bars = ax.bar([str(w) for w in widths], final_acc, color=colors, \n",
    "              edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Hidden Layer Width', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Final Test Accuracy by Hidden Layer Width', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"final_accuracy_summary.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)\n",
    "\n",
    "# ------------- Dataset Summary Table/Diagram -------------\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "ax.axis('off')\n",
    "\n",
    "# Dataset info table\n",
    "dataset_info = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Total Samples', f'{n_samples}'],\n",
    "    ['Training Samples', f'{len(X_train)}'],\n",
    "    ['Test Samples', f'{len(X_test)}'],\n",
    "    ['Number of Features', f'{n_features}'],\n",
    "    ['Number of Classes', f'{n_classes}'],\n",
    "    ['Test Size Ratio', '0.2 (20%)'],\n",
    "    ['Preprocessing', 'StandardScaler'],\n",
    "    ['Class Distribution', 'Stratified'],\n",
    "    ['Random Seed', f'{RND}']\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=dataset_info, cellLoc='center', loc='center',\n",
    "                colWidths=[0.4, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Header styling\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(dataset_info)):\n",
    "    for j in range(2):\n",
    "        table[(i, j)].set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n",
    "\n",
    "plt.title('Dataset Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "p = os.path.join(output_dir, \"dataset_summary.png\")\n",
    "fig.savefig(p, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "figure_paths.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febfe99",
   "metadata": {},
   "source": [
    "**PDF Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cc799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Outputs in: mlp_width_outputs\n",
      "Open: mlp_width_outputs\\MLP_width_tutorial.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_path = os.path.join(output_dir, \"MLP_width_tutorial.pdf\")\n",
    "pp = PdfPages(pdf_path)\n",
    "\n",
    "def add_text_page(title, paragraphs):\n",
    "    fig = plt.figure(figsize=(8.27, 11.69))\n",
    "    plt.axis('off')\n",
    "    plt.text(0.02, 0.96, title, fontsize=16, weight='bold')\n",
    "    y = 0.92\n",
    "    for para in paragraphs:\n",
    "        wrapped = textwrap.fill(para, 90)\n",
    "        plt.text(0.02, y, wrapped, fontsize=10, va='top', family='serif')\n",
    "        y -= 0.08 * (wrapped.count('\\n') + 3)\n",
    "        if y < 0.08:\n",
    "            break\n",
    "    pp.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "title = \"How Hidden Layer Width Influences the Performance of an MLP\"\n",
    "intro = (\"This tutorial studies how the number of neurons in a single hidden layer \"\n",
    "         \"affects learning, generalisation and training behaviour. We test widths: \" + \", \".join(map(str,widths)) + \".\")\n",
    "background = (\"Key ideas: width controls model capacity. Small widths lead to underfitting, \"\n",
    "              \"very large widths often lead to overfitting. The universal approximation theorem \"\n",
    "              \"guarantees representational ability with sufficient width but says nothing about generalisation.\")\n",
    "method = (\"Experimental setup: synthetic classification dataset (sklearn.make_classification), \"\n",
    "          f\"{n_samples} examples, {n_features} features, {n_classes} classes. StandardScaler applied. \"\n",
    "          f\"Each MLP trained for {epochs} epochs with Adam optimizer (lr=0.001). Metrics: train/test accuracy and training loss.\")\n",
    "results_text = (\"See figures: per-width accuracy & loss curves, and a summary final-accuracy bar chart. \"\n",
    "                \"Typical finding: medium widths (e.g. 32–64) often provide optimal trade-off; very wide layers \"\n",
    "                \"achieve low training loss but exhibit larger train-test gap (overfitting).\")\n",
    "conclusion = (\"Practical rules: start with modest widths, increase only if underfitting detected, apply L2 regularisation \"\n",
    "              \"for large widths, consider depth vs width trade-offs for optimal architecture design.\")\n",
    "refs = (\"References: Hornik et al. (1989); Lu et al. (2017); Montúfar et al. (2014); Jacot et al. (2018).\")\n",
    "\n",
    "add_text_page(title, [intro, background, method, results_text, conclusion, refs])\n",
    "\n",
    "for p in figure_paths:\n",
    "    img = plt.imread(p)\n",
    "    fig = plt.figure(figsize=(8.27, 11.69))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    pp.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "pp.close()\n",
    "print(\"All done. Outputs in:\", output_dir)\n",
    "print(\"Open:\", pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97e1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
